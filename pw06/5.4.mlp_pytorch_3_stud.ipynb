{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-suffering",
   "metadata": {},
   "source": [
    "## Practical Work - 06 (Part.1)\n",
    "\n",
    "#### Objectives\n",
    "\n",
    "The main objective of this PW is to understand the overall functioning of the DL-frameworks\n",
    "PyTorch and Keras API for TensorFlow, as well as the visualisation tool TensorBoard. In\n",
    "particular we want to con\u001Cgure a multi-layer perceptron (MLP) and train it on the CIFAR10\n",
    "image dataset using both frameworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "educational-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:22:45.145885100Z",
     "start_time": "2024-03-28T19:22:44.800971100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd9ac65-cf2d-4571-93ce-aeb172cbfbd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:22:53.307558400Z",
     "start_time": "2024-03-28T19:22:45.144897300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#only at first execution data is downloaded, because it is saved in subfolder ../week1/data; \n",
    "#note the relative path to the 01.learning-optimization to avoid multiple downloads\n",
    "\n",
    "transformer = torchvision.transforms.ToTensor()\n",
    "\n",
    "data_set = 'CIFAR'\n",
    "\n",
    "if data_set == 'CIFAR':\n",
    "    training_data = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transformer\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transformer\n",
    "    )\n",
    "\n",
    "    #labels for CIFAR (just for compatibility reasons)\n",
    "    labels_map = {\n",
    "        0: \"Airplane\",\n",
    "        1: \"Automobile\",\n",
    "        2: \"Bird\",\n",
    "        3: \"Cat\",\n",
    "        4: \"Deer\",\n",
    "        5: \"Dog\",\n",
    "        6: \"Frog\",\n",
    "        7: \"Horse\",\n",
    "        8: \"Ship\",\n",
    "        9: \"Truck\",\n",
    "    }\n",
    "elif data_set == 'FashionMNIST':\n",
    "    training_data = torchvision.datasets.FashionMNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transformer\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.FashionMNIST(\n",
    "        root=\"./data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transformer\n",
    "    )\n",
    "\n",
    "    #labels for FashionMNIST\n",
    "    labels_map = {\n",
    "        0: \"T-Shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "print(isinstance(training_data, Dataset))\n",
    "print(hasattr(training_data, '__len__'))\n",
    "print(hasattr(training_data, '__getitem__'))\n",
    "print(hasattr(training_data, 'transform'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:22:53.408078500Z",
     "start_time": "2024-03-28T19:22:53.341500300Z"
    }
   },
   "id": "44eee9a3fc79aa43",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 32, 32])\n",
      "torch.Size([256])\n",
      "tensor([0, 3, 7, 1, 5, 5, 1, 6, 8, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=256, shuffle=True)\n",
    "\n",
    "data_iterator = iter(train_loader)\n",
    "\n",
    "images, labels = next(data_iterator)\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "print(labels[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:22:54.381271400Z",
     "start_time": "2024-03-28T19:22:53.462980600Z"
    }
   },
   "id": "f2cd529999f7a59f",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae4fdc4a0cee5289"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#original data\n",
    "\n",
    "#print(\"type of original data: %r\" %  training_data.data.dtype)\n",
    "#print(\"min value of original data: %r\" %  np.min(training_data.data.item()))\n",
    "#print(\"max value of original data: %r\" %  np.max(training_data.data.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:22:54.473369100Z",
     "start_time": "2024-03-28T19:22:54.369234300Z"
    }
   },
   "id": "cfd7d32f392d0a87",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "type of loaded data: torch.float32\n",
      "min value of loaded data: 0.0\n",
      "max value of loaded data: 1.0\n"
     ]
    }
   ],
   "source": [
    "#loaded data\n",
    "print(\"\\ntype of loaded data: %r\" % images.dtype)\n",
    "print(\"min value of loaded data: %r\" % torch.min(images).item())\n",
    "print(\"max value of loaded data: %r\" % torch.max(images).item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:22:54.574322500Z",
     "start_time": "2024-03-28T19:22:54.406490900Z"
    }
   },
   "id": "6858aa5c510bc471",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of images batches:\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(256, 3, 32, 32)\n",
      "(80, 3, 32, 32)\n",
      "time for transforming entire batch: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "print(\"shape of images batches:\")\n",
    "for batch_iter in data_iterator:\n",
    "    print(batch_iter[0].numpy().shape)\n",
    "\n",
    "end = time.time()\n",
    "print(\"time for transforming entire batch: %2.1f s\" % (end - start))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:23.162018200Z",
     "start_time": "2024-03-28T19:22:54.459994300Z"
    }
   },
   "id": "afc95c0831309a7c",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "5d7484b6-292a-465f-9077-6b32e1847042",
   "metadata": {},
   "source": [
    "### Create custom dataset\n",
    "\n",
    "The following dataset does the type conversion and normalisation only once in the constructor and then only gives back the prepared images. It uses our previous method `prepare_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "359ac812-b47b-40ab-9450-bf0091dcb396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:23.265027400Z",
     "start_time": "2024-03-28T19:23:23.142339600Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"owns dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, classes=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), min_max_normalise=1, flatten=0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        dataset -- a tuple with the [images, labels] of the original dataset\n",
    "        classes -- list of classes to use for training (at least two classes must be given)\n",
    "        min_max_normalise -- whether to do min-max-normalisation (1) or rescaling (0)\n",
    "        flatten -- whether to flatten the 28x28 image to single row (=1);\n",
    "\n",
    "        \"\"\"\n",
    "        self.prepare_data(dataset, classes, min_max_normalise, flatten)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #add the missing map-dimension\n",
    "        return self.x_sel[idx], self.y_sel[idx]\n",
    "\n",
    "    def prepare_data(self, dataset, classes, min_max_normalise, flatten):\n",
    "        x = dataset[0]\n",
    "        y = dataset[1]\n",
    "\n",
    "        if len(classes) < len(labels_map):\n",
    "            for label in classes:\n",
    "                print('labels chosen are: %r' % labels_map[label.item()])\n",
    "\n",
    "        ind_sel = torch.isin(y, classes)\n",
    "\n",
    "        x_sel = torch.zeros(x[ind_sel, :].shape, dtype=torch.float)\n",
    "        x_sel.copy_(x[ind_sel, :])\n",
    "        y_sel = torch.zeros(y[ind_sel].shape, dtype=y.dtype)\n",
    "        y_sel.copy_(y[ind_sel])\n",
    "\n",
    "        #replace the labels such that they are in successive order\n",
    "        for i0 in range(0, len(classes)):\n",
    "            if i0 != classes[i0]:\n",
    "                y_sel[y_sel == classes[i0]] = i0\n",
    "\n",
    "        #we give y back as simple vector -> simplifies handling below\n",
    "        #y_sel = np.reshape(y_sel, (-1,1))\n",
    "\n",
    "        #do train and test split\n",
    "        self.num_samples = x_sel.shape[0]\n",
    "\n",
    "        #perform normalisation, take care of converting data type to float!\n",
    "        xmax, xmin = torch.max(x_sel), torch.min(x_sel)\n",
    "\n",
    "        if min_max_normalise:\n",
    "            x_sel = 2 * (x_sel - xmin) / (xmax - xmin) - 1\n",
    "        else:\n",
    "            x_sel = x_sel / xmax\n",
    "\n",
    "        if flatten:\n",
    "            m = x_sel.shape[0]\n",
    "            x_sel = x_sel.reshape([m, -1])\n",
    "\n",
    "        self.x_sel = torch.unsqueeze(x_sel, 1)\n",
    "        self.y_sel = y_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be8bd0-09fa-4f85-8281-210abf5214ec",
   "metadata": {},
   "source": [
    "### Set output directory for tensorboard\n",
    "\n",
    "This folder is relative to the working path on the hard disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0d531c-0b55-4c68-bd68-efff3e3a63b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:23.460694Z",
     "start_time": "2024-03-28T19:23:23.256241800Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('tensorboard/cifar_experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a0282-9fa3-4288-a443-39d4dcf1cb45",
   "metadata": {},
   "source": [
    "### we can send images to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe543be1-774a-4503-a354-cd9d335eddc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:23.800612100Z",
     "start_time": "2024-03-28T19:23:23.461697500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.data.shape)\n",
    "\n",
    "training_data_tensor = torch.tensor(training_data.data)\n",
    "\n",
    "# get some random training images (add map dimension at position 1)\n",
    "my_images = training_data_tensor[:20].permute(0, 3, 1, 2)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(my_images)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('a_set_of_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd5fb-d25e-4fe5-906e-386c955898a1",
   "metadata": {},
   "source": [
    "### Class NeuralNetwork\n",
    "\n",
    "This class constructs a Multilayer Perceptron with a configurable number of hidden layers. Cost function is CE. The method $propagate()$ returns the prediction $$ \\hat{y}^{(i)}=h_\\theta(\\mathbf{x}^{(i)}) $$ on the input data (can be a n x 784 matrix of n images) and $back\\_propagate()$ determines the gradients of the cost function with respect to the parameters (weights and bias for all layers) $$ \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta}) $$\n",
    "The method $gradient\\_descend()$ finally does the correction of the parameters with a step in the negative gradient direction, weighted with the learning rate $$\\alpha$$ for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c173bc6-1969-4bd7-87fe-b6807f8529d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:24.104776300Z",
     "start_time": "2024-03-28T19:23:23.926803700Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    MLP class handling the layers and doing all propagation and back propagation steps\n",
    "    all hidden layers are dense (with ReLU activation) and the last layer is softmax\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, list_num_neurons):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        list_num_neurons -- list of layer sizes including in- and output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model = torch.nn.Sequential()\n",
    "        #now we require a flatten tensor\n",
    "        self.model.add_module('flatten', torch.nn.Flatten(start_dim=1, end_dim=-1))\n",
    "        #first construct dense layers\n",
    "        for i0 in range(len(list_num_neurons) - 2):\n",
    "            self.model.add_module('dense' + str(i0), torch.nn.Linear(list_num_neurons[i0], list_num_neurons[i0 + 1]))\n",
    "            self.model.add_module('act' + str(i0), torch.nn.ReLU())\n",
    "\n",
    "        #finally add softmax layer\n",
    "        self.model.add_module('dense' + str(i0 + 1), torch.nn.Linear(list_num_neurons[-2], list_num_neurons[-1]))\n",
    "        self.model.add_module('act' + str(i0 + 1), torch.nn.Softmax(dim=1))\n",
    "\n",
    "        self.cost_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        #used to save results\n",
    "        self.result_data = torch.tensor([])\n",
    "\n",
    "        #we keep a global step counter, thus that optimise can be called \n",
    "        #several times with different settings\n",
    "        self.epoch_counter = 0\n",
    "\n",
    "    def propagate(self, x):\n",
    "        \"\"\"\n",
    "        calculates the function estimation based on current parameters\n",
    "        \"\"\"\n",
    "        y_pred = self.model(x)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def back_propagate(self, cost):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on expected output y\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"\n",
    "        #set gradient values to zero\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "    def cost_funct(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the MSE loss function\n",
    "        \"\"\"\n",
    "        cost = self.cost_fn(y_pred, y)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def calc_error(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        get error information\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "\n",
    "        y_pred_argmax = torch.argmax(y_pred, dim=1)\n",
    "        error = torch.sum(y != y_pred_argmax) / m\n",
    "\n",
    "        return error\n",
    "\n",
    "    def append_result(self):\n",
    "        \"\"\"\n",
    "        append cost and error data to output array\n",
    "        \"\"\"\n",
    "        #this takes quite a long time (transform is applied to all images) but is only executed once \n",
    "        #then the images are available for quick execution of propagation step\n",
    "        if self.epoch_counter == 0:\n",
    "            # dataloaders (we use original set (training/test_data); own data has to realize the abstract class representing 'Dataset'\n",
    "            train_loader = torch.utils.data.DataLoader(self.data['train'], batch_size=len(self.data['train']),\n",
    "                                                       shuffle=False)\n",
    "            train_iterator = iter(train_loader)\n",
    "            self.train_images, self.train_labels = next(train_iterator)\n",
    "\n",
    "            valid_loader = torch.utils.data.DataLoader(self.data['valid'], batch_size=len(self.data['valid']),\n",
    "                                                       shuffle=False)\n",
    "            valid_iterator = iter(valid_loader)\n",
    "            self.valid_images, self.valid_labels = next(valid_iterator)\n",
    "\n",
    "        # determine cost and error functions for train and validation data\n",
    "        y_pred_train = self.propagate(self.train_images)\n",
    "        y_pred_val = self.propagate(self.valid_images)\n",
    "\n",
    "        res_data = torch.tensor([[self.cost_funct(y_pred_train, self.train_labels),\n",
    "                                  self.calc_error(y_pred_train, self.train_labels),\n",
    "                                  self.cost_funct(y_pred_val, self.valid_labels),\n",
    "                                  self.calc_error(y_pred_val, self.valid_labels)]])\n",
    "\n",
    "        self.result_data = torch.cat((self.result_data, res_data), 0)\n",
    "\n",
    "        #send data to tensorboard   \n",
    "        writer.add_scalars('loss', {'train': res_data[0, 0].item(), \\\n",
    "                                    'validate': res_data[0, 2].item()}, self.epoch_counter)\n",
    "\n",
    "        writer.add_scalars('error', {'train': res_data[0, 1].item(), \\\n",
    "                                     'validate': res_data[0, 3].item()}, self.epoch_counter)\n",
    "\n",
    "        #increase epoch counter here (used for plot routines below)\n",
    "        self.epoch_counter += 1\n",
    "\n",
    "        return res_data\n",
    "\n",
    "    def optimise(self, data, epochs, alpha, batch_size=0, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with NORMALISED data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        batch_size -- size of batches (1 = SGD, 1 < .. < n = mini-batch)\n",
    "        debug -- integer value; get info on gradient descend step every debug-step (0 -> no output)\n",
    "        \"\"\"\n",
    "        #access to data from other methods\n",
    "        self.data = data\n",
    "\n",
    "        #we define the optimiser\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha, momentum=0.)\n",
    "        #self.optimizer = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "        # dataloader for training image\n",
    "        train_loader = torch.utils.data.DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # save results before 1st step\n",
    "        if self.epoch_counter == 0:\n",
    "            res_data = self.append_result()\n",
    "\n",
    "        for i0 in range(0, epochs):\n",
    "            #measure time for one epoch\n",
    "            start = time.time()\n",
    "            #setup loop over all batchs\n",
    "            data_iterator = iter(train_loader)\n",
    "            for batch_iter in data_iterator:\n",
    "                #do prediction\n",
    "                y_pred = self.propagate(batch_iter[0])\n",
    "                #determine the loss \n",
    "                cost = self.cost_funct(y_pred, batch_iter[1])\n",
    "                #determine the error\n",
    "                self.back_propagate(cost)\n",
    "                #do the correction step\n",
    "                self.gradient_descend(alpha)\n",
    "\n",
    "            #save result\n",
    "            res_data = self.append_result()\n",
    "\n",
    "            #end of time measurement\n",
    "            end = time.time()\n",
    "\n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('result after %d epochs (dt=%1.2f s)' % (self.epoch_counter - 1, end - start))\n",
    "\n",
    "        if debug:\n",
    "            print('result after %d epochs, train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                  % (self.epoch_counter - 1, res_data[0, 0].item(), res_data[0, 1].item(), \\\n",
    "                     res_data[0, 2].item(), res_data[0, 3].item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfae34-9c4a-461f-b06f-d9abc093b775",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "#### We split the creation and optimisation\n",
    "\n",
    "The cells below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. The classes (from 2 to 10) can be chosen via the `classes` list. This method can be called several times in a row with different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e6bc169-8a62-4fc8-aab0-d88ab4b774d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:31.616050200Z",
     "start_time": "2024-03-28T19:23:24.100808700Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose the categories\n",
    "classes = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "#split data in train and validation\n",
    "validation_size = 0.2\n",
    "\n",
    "#further split in train and validation data\n",
    "valid_ind = int(len(training_data_tensor) * (1 - validation_size))\n",
    "\n",
    "#print(training_data_tensor[:1,:])\n",
    "\n",
    "#create custom training and validation data set\n",
    "train_dataset = MyDataset([training_data_tensor[:valid_ind, :], torch.tensor(training_data.targets)[:valid_ind]],\n",
    "                          classes=classes)\n",
    "valid_dataset = MyDataset([training_data_tensor[valid_ind:, :], torch.tensor(training_data.targets)[valid_ind:]],\n",
    "                          classes=classes)\n",
    "\n",
    "#data is arranged as dictionary with quick access through respective keys\n",
    "data = {'train': train_dataset, 'valid': valid_dataset}\n",
    "\n",
    "#choose the hyperparameters you want to use for the initialisation\n",
    "size_in = train_dataset[0][0].flatten().shape[0]  #access to first image in torch.Subset train_data \n",
    "size_out = 10\n",
    "list_num_neurons = [size_in, 100, size_out];\n",
    "NNet = NeuralNetwork(list_num_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8abd4-b1bf-4e66-a2f8-4c689d945487",
   "metadata": {},
   "source": [
    "### Send the graph to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71e92ed4-2f51-4b3f-a8d0-562c4ee6eb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:32.256362800Z",
     "start_time": "2024-03-28T19:23:31.661042100Z"
    }
   },
   "outputs": [],
   "source": [
    "writer.add_graph(NNet.model, my_images.float())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa03736-995c-49c5-8a36-ad1a23ce0436",
   "metadata": {},
   "source": [
    "### Add data for embedding to tensorboard \n",
    "\n",
    "Its more a gadget but nice to see \n",
    "(you may have to reload the tensorboard page or even restart the tensorboard in the console to see the `projector` icon on the task bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f53f13241b635782",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T19:23:34.796925Z",
     "start_time": "2024-03-28T19:23:32.264353700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "#get a larger set of images and labels\n",
    "num_samples = 200\n",
    "my_images = training_data_tensor[:num_samples]\n",
    "my_labels = torch.tensor(training_data.targets)[:num_samples]\n",
    "# log embeddings\n",
    "writer.add_embedding(my_images.reshape(-1, 3 * 32 * 32),\n",
    "                     metadata=my_labels,\n",
    "                     label_img=my_images.permute(0, 3, 1, 2))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea58b8-3e3e-4132-8f4e-703b93331025",
   "metadata": {},
   "source": [
    "### Now run the training and observe the scalar output on tensorboard\n",
    "\n",
    "We see, that we can keep the code clean of any output and rely completely on tensorboard for that"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result after 1 epochs (dt=5.09 s)\n",
      "result after 6 epochs (dt=7.75 s)\n",
      "result after 10 epochs, train: cost 2.08009, error 0.61350 ; validation: cost 2.09084, error 0.62830\n",
      "test error rate: 61.89 % out of 10000\n",
      "torch.Size([6189, 1, 32, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "#choose the hyperparameters you want to use for training\n",
    "epochs = 10\n",
    "batchsize = 256\n",
    "learning_rate = 0.05\n",
    "NNet.optimise(data, epochs, learning_rate, batchsize, debug=5)\n",
    "\n",
    "#also prepare the test dataset\n",
    "test_dataset = MyDataset([torch.tensor(test_data.data), torch.tensor(test_data.targets)], classes=classes)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "test_iterator = iter(test_loader)\n",
    "test_images, test_labels = next(test_iterator)\n",
    "\n",
    "y_pred = torch.argmax(NNet.propagate(test_images), axis=1)\n",
    "false_classifications = test_images[(y_pred != test_labels)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100 * false_classifications.shape[0] / y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T19:24:54.238084200Z",
     "start_time": "2024-03-28T19:23:34.803756300Z"
    }
   },
   "id": "3f5e5245-456f-4026-8978-8a1b8776ee63",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results on tensorboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee7a2f2676c83a76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Here is the results of the tensorboard*\n",
    "\n",
    "1.  \n",
    "This TensorBoard screenshot displays a grid of images labeled \"a_set_of_fashion_mnist_images\".\n",
    "![](images/cifar_part10.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811d0624cf45a239"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "2.\n",
    "The screenshot from TensorBoard displays two time series graphs, \"error\" and \"loss\", across training epochs for a machine learning model. The \"error\" graph, at the top, shows two lines representing training and validation error: the training error (purple line) appears to start higher and decrease slightly as training progresses, while the validation error (orange line) starts lower but increases, suggesting a potential overfitting issue where the model is learning the training data too closely but not generalizing well to unseen data.\n",
    "![](images/cifar_part12.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6989ab87696b179"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "3.\n",
    "This image features a high-dimensional data visualization using PCA (Principal Component Analysis). It shows a scatter plot with points colored by ten different labels, which likely represent different classes in a dataset. The plot helps in understanding the distribution and separation of classes in a reduced dimensional space, although the points do not seem to be distinctly clustered by class, indicating a potential overlap in feature space.\n",
    "![](images/cifar_part13.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fa8b85c9300c0ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.\n",
    "This image displays a simple model architecture within TensorBoard, consisting of a sequential model with an input layer and an output layer. The sequential notation suggests that this model processes data through a series of layers that are stacked linearly.\n",
    "![](images/cifar_part14.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5af96b600f49645"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
