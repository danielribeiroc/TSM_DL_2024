{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187bc0fb-4972-4653-a85d-6f5c2fe9c039",
   "metadata": {},
   "source": [
    "## Compact MLP\n",
    "\n",
    "This script is to illustrate the architecture of an MLP and is supposed to help understanding the matrix-wise mulitplication of input-vetor/action and weights. In addition, the backpropagaten algorithm will be implemented according to the formulas given in chapter 4.5 of the lecture notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "freelance-greensboro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:24:32.973152Z",
     "start_time": "2024-03-12T14:24:32.169407Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cae4fc-775d-4df5-b56e-8e5b947f423a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975b6612-425c-4e33-9c7c-cfdfe43b02af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:24:35.035755200Z",
     "start_time": "2024-03-12T14:24:35.012023300Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_value(val, str = 'var'):\n",
    "    print('\\n %s is of shape %r and has values\\n %r' % (str, val.shape, val))\n",
    "\n",
    "\n",
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def dsig(z):\n",
    "    return sig(z)*(1-sig(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13fb7f-c7a2-490d-9c06-1f7713634828",
   "metadata": {},
   "source": [
    "## Class NeuralNetwork\n",
    "\n",
    "Implementation of a MLP with two layers, where the weights and biases are given as arguments in the constructor. The backpropagation is not implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fc8260-4492-4d72-8d19-0d5ead10b63a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:24:37.715197600Z",
     "start_time": "2024-03-12T14:24:37.685685400Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    simple MLP class (first illustration purposes only)\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, W1, b1, W2, b2):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        W1, W2 -- weight matrices of the two layers\n",
    "        b1, b2 -- biases of the two layers\n",
    "        \"\"\"\n",
    "        self.W1 = W1\n",
    "        self.b1 = b1\n",
    "        self.W2 = W2\n",
    "        self.b2 = b2\n",
    "\n",
    "\n",
    "    def activation_function(self, z):\n",
    "        \"\"\"\n",
    "        apply activation function defined above\n",
    "        \"\"\"\n",
    "        return sig(z)\n",
    "        \n",
    "        \n",
    "    def propagate(self, x):\n",
    "        \"\"\"\n",
    "        predicted outcome for x\n",
    "        \"\"\"\n",
    "        #first layer\n",
    "        z1 = self.W1 @ x + self.b1\n",
    "        a1 = self.activation_function(z1)\n",
    "        \n",
    "        #second layer\n",
    "        z2 = self.W2 @ a1 + self.b2\n",
    "        a2 = self.activation_function(z2)\n",
    "\n",
    "        self.a1 = a1 #access from outside\n",
    "        y_pred = a2\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def back_propagate(self):\n",
    "        return 0\n",
    " \n",
    "    \n",
    "    def cost_funct(self, y_pred):\n",
    "        \"\"\"\n",
    "        MSE loss with y == 0\n",
    "        \"\"\"\n",
    "        #in case we have a batch\n",
    "        m = y_pred.shape[1] \n",
    "\n",
    "        cost = np.sum(y_pred**2)/(2*m)\n",
    "        \n",
    "        return cost\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea6a5c-c3b4-42a5-9442-c572833dde77",
   "metadata": {},
   "source": [
    "## Propagation step\n",
    "\n",
    "Observe in detail the sizes of the input vector, the weights and biases and the corresponding activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noble-contrary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:36:51.534301200Z",
     "start_time": "2024-03-12T14:36:51.463265200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " a1 is of shape (4, 1) and has values\n",
      " array([[0.77729986],\n",
      "       [0.5       ],\n",
      "       [0.5621765 ],\n",
      "       [0.18242552]])\n",
      "\n",
      " a2 is of shape (3, 1) and has values\n",
      " array([[0.64168794],\n",
      "       [0.49347802],\n",
      "       [0.55467851]])\n",
      "\n",
      " L is of shape () and has values\n",
      " 0.4814761106026716\n"
     ]
    }
   ],
   "source": [
    "#IMPORTANT: pay attention to define all array as float i.e., not using '1' but '1.0' (dtype=np.float64)\n",
    "x = np.array([[1, -1, -0.5]]).T\n",
    "\n",
    "W1 = np.array([[1, -1, -0.5],[1, 0.5, -1],[-0.2, 0.3, 0.5], [-1, 1, 1]])\n",
    "b1 = np.array([[-1., -1., 1., 1.]]).T\n",
    "\n",
    "W2 = np.array([[1, 0.5, -1, -1],[1, 0.5, -1, -0.5],[1, 1, -0.2, 0.3]])\n",
    "b2 = np.array([[0.3, -0.4, -1]]).T\n",
    "\n",
    "NNet = NeuralNetwork(W1, b1, W2, b2)\n",
    "\n",
    "y_pred = NNet.propagate(x)\n",
    "L = NNet.cost_funct(y_pred)\n",
    "\n",
    "print_value(NNet.a1,'a1')\n",
    "print_value(y_pred,'a2')\n",
    "print_value(L,'L')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a7221-a992-4055-bffe-2092cfacacbb",
   "metadata": {},
   "source": [
    "## Propagation step (batch)\n",
    "\n",
    "Implement a copy of the above cell but with a full batch (minimum size 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bfc004-196f-4a1b-98f7-f38ec827c9b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:39:19.551086100Z",
     "start_time": "2024-03-12T15:39:19.526025200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " a1 is of shape (4, 2) and has values\n",
      " array([[0.77729986, 0.77729986],\n",
      "       [0.5       , 0.5       ],\n",
      "       [0.5621765 , 0.5621765 ],\n",
      "       [0.18242552, 0.18242552]])\n",
      "\n",
      " a2 is of shape (3, 2) and has values\n",
      " array([[0.64168794, 0.64168794],\n",
      "       [0.49347802, 0.49347802],\n",
      "       [0.55467851, 0.55467851]])\n",
      "\n",
      " L is of shape () and has values\n",
      " 0.4814761106026716\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "X = np.array([[1, -1, -0.5],[1,-1,-0.5]]).T\n",
    " \n",
    "### END YOUR CODE ###\n",
    "\n",
    "W1 = np.array([[1, -1, -0.5],[1, 0.5, -1],[-0.2, 0.3, 0.5], [-1, 1, 1]])\n",
    "b1 = np.array([[-1., -1., 1., 1.]]).T\n",
    "\n",
    "W2 = np.array([[1, 0.5, -1, -1],[1, 0.5, -1, -0.5],[1, 1, -0.2, 0.3]])\n",
    "b2 = np.array([[0.3, -0.4, -1]]).T\n",
    "\n",
    "NNet = NeuralNetwork(W1, b1, W2, b2)\n",
    "\n",
    "y_pred = NNet.propagate(X)\n",
    "L = NNet.cost_funct(y_pred)\n",
    "\n",
    "print_value(NNet.a1,'a1')\n",
    "print_value(y_pred,'a2')\n",
    "print_value(L,'L')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ac514-e3a6-4153-ac6d-eb59555d6e73",
   "metadata": {},
   "source": [
    "## Backpropagation step\n",
    "\n",
    "Implement using the set of formulas in equation 9, chapter 4.5 in the lecture notes the backpropagation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642ef7ed-a17c-4cbb-b4ec-1b5fb319e1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:59:10.706971700Z",
     "start_time": "2024-03-12T15:59:10.647954500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dL_dW2 is of shape (3, 4) and has values\n",
      " array([[0.11468266, 0.0737699 , 0.08294341, 0.02691502],\n",
      "       [0.09587878, 0.06167426, 0.06934364, 0.02250192],\n",
      "       [0.10649885, 0.06850564, 0.07702452, 0.02499436]])\n",
      "\n",
      " dL_db2 is of shape (3, 1) and has values\n",
      " array([[0.1475398 ],\n",
      "       [0.12334851],\n",
      "       [0.13701128]])\n",
      "\n",
      " dL_dW1 is of shape (4, 3) and has values\n",
      " array([[ 0.07060937, -0.07060937, -0.03530469],\n",
      "       [ 0.06811386, -0.06811386, -0.03405693],\n",
      "       [-0.07341948,  0.07341948,  0.03670974],\n",
      "       [-0.02507311,  0.02507311,  0.01253655]])\n",
      "\n",
      " dL_db1 is of shape (1,) and has values\n",
      " array([0.04023065])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1., -1., -0.5]]).T\n",
    "\n",
    "W1 = np.array([[1, -1, -0.5],[1, 0.5, -1],[-0.2, 0.3, 0.5], [-1, 1, 1]])\n",
    "b1 = np.array([[-1., -1., 1., 1.]]).T\n",
    "\n",
    "W2 = np.array([[1, 0.5, -1, -1],[1, 0.5, -1, -0.5],[1, 1, -0.2, 0.3]])\n",
    "b2 = np.array([[0.3, -0.4, -1]]).T\n",
    "\n",
    "#here we rewrite the formulas explicitly outside NNet to have easier access to all values\n",
    "#forward path\n",
    "z1 = W1 @ x + b1\n",
    "a1 = sig(z1)\n",
    "\n",
    "#second layer\n",
    "z2 = W2 @ a1 + b2\n",
    "a2 = sig(z2)\n",
    "\n",
    "#backward: this is the derivative of the cost function (1/3 due to three output values)\n",
    "dL_da2 = a2\n",
    "\n",
    "### START YOUR CODE ###\n",
    "dg2_dz = dsig(z2)\n",
    "dL_dz2 = dg2_dz * dL_da2\n",
    "dL_dW2 = dL_dz2 @ a1.T\n",
    "dL_db2 = np.sum(dL_dz2, axis=1,keepdims=True)\n",
    "dL_da1 = W2.T @ dL_dz2\n",
    "\n",
    "dg1_dz = dsig(z1)\n",
    "dL_dz1 = dg1_dz * dL_da1\n",
    "dL_dW1 = dL_dz1 @ x.T\n",
    "dL_db1 = sum(dL_dz1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print_value(dL_dW2,'dL_dW2')\n",
    "print_value(dL_db2,'dL_db2')\n",
    "print_value(dL_dW1,'dL_dW1')\n",
    "print_value(dL_db1,'dL_db1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd5057-b41e-4d4c-b9d2-4cdc9ace516d",
   "metadata": {},
   "source": [
    "## Backpropagation step (batch)\n",
    "\n",
    "Implement a copy of the above cell but with a full batch (minimum size 2). Use formulas in equation 10, chapter 4.5.5 in the lecture notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab180ecc-aa81-4d3b-a025-850fc91406d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:59:40.200888100Z",
     "start_time": "2024-03-12T15:59:40.163990400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dL_dW2 is of shape (3, 4) and has values\n",
      " array([[0.11468266, 0.0737699 , 0.08294341, 0.02691502],\n",
      "       [0.09587878, 0.06167426, 0.06934364, 0.02250192],\n",
      "       [0.10649885, 0.06850564, 0.07702452, 0.02499436]])\n",
      "\n",
      " dL_db2 is of shape (3, 1) and has values\n",
      " array([[0.1475398 ],\n",
      "       [0.12334851],\n",
      "       [0.13701128]])\n",
      "\n",
      " dL_dW1 is of shape (4, 3) and has values\n",
      " array([[ 0.07060937, -0.07060937, -0.03530469],\n",
      "       [ 0.06811386, -0.06811386, -0.03405693],\n",
      "       [-0.07341948,  0.07341948,  0.03670974],\n",
      "       [-0.02507311,  0.02507311,  0.01253655]])\n",
      "\n",
      " dL_db1 is of shape (4, 1) and has values\n",
      " array([[ 0.07060937],\n",
      "       [ 0.06811386],\n",
      "       [-0.07341948],\n",
      "       [-0.02507311]])\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "X = np.array([[1, -1, -0.5],[1,-1,-0.5]]).T\n",
    "### END YOUR CODE ###\n",
    "\n",
    "W1 = np.array([[1, -1, -0.5],[1, 0.5, -1],[-0.2, 0.3, 0.5], [-1, 1, 1]])\n",
    "b1 = np.array([[-1., -1., 1., 1.]]).T\n",
    "\n",
    "W2 = np.array([[1, 0.5, -1, -1],[1, 0.5, -1, -0.5],[1, 1, -0.2, 0.3]])\n",
    "b2 = np.array([[0.3, -0.4, -1]]).T\n",
    "\n",
    "#here we rewrite the formulas explicitly outside NNet to have easier access to all values\n",
    "#forward path\n",
    "z1 = W1 @ X + b1\n",
    "a1 = sig(z1)\n",
    "\n",
    "#second layer\n",
    "z2 = W2 @ a1 + b2\n",
    "a2 = sig(z2)\n",
    "\n",
    "#backward: this is the derivative of the cost function (1/m due to size of batch)\n",
    "m = a2.shape[1]\n",
    "dL_da2 = a2/m\n",
    "\n",
    "### START YOUR CODE ###\n",
    "dg2_dz = dsig(z2)\n",
    "dL_dz2 = dg2_dz * dL_da2 \n",
    "dL_dW2 = dL_dz2 @ a1.T\n",
    "dL_db2 = np.sum(dL_dz2, axis=1, keepdims=True)\n",
    "dL_da1 = W2.T @ dL_dz2\n",
    "\n",
    "dg1_dz = dsig(z1)\n",
    "dL_dz1 = dg1_dz * dL_da1\n",
    "dL_dW1 = dL_dz1 @ X.T\n",
    "dL_db1 = np.sum(dL_dz1, axis=1, keepdims=True)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print_value(dL_dW2,'dL_dW2')\n",
    "print_value(dL_db2,'dL_db2')\n",
    "print_value(dL_dW1,'dL_dW1')\n",
    "print_value(dL_db1,'dL_db1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996b6a4-8d2f-4d38-83f4-e621800d807e",
   "metadata": {},
   "source": [
    "## Numerical gradient check\n",
    "\n",
    "We calculate the derivatives numerically and compare the results with our analytical calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dietary-parcel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T16:00:03.543018300Z",
     "start_time": "2024-03-12T16:00:03.521696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dL_dW2 (numerical check) is of shape (3, 4) and has values\n",
      " array([[0.11468267, 0.0737699 , 0.08294341, 0.02691502],\n",
      "       [0.0958788 , 0.06167427, 0.06934365, 0.02250192],\n",
      "       [0.10649887, 0.06850565, 0.07702453, 0.02499436]])\n",
      "\n",
      " dL_db2 (numerical check) is of shape (3, 1) and has values\n",
      " array([[0.1475398 ],\n",
      "       [0.12334855],\n",
      "       [0.13701131]])\n",
      "\n",
      " dL_dW1 (numerical check) is of shape (4, 3) and has values\n",
      " array([[ 0.07060935, -0.07060939, -0.03530469],\n",
      "       [ 0.06811386, -0.06811386, -0.03405693],\n",
      "       [-0.07341947,  0.07341948,  0.03670974],\n",
      "       [-0.02507312,  0.0250731 ,  0.01253655]])\n",
      "\n",
      " dL_db1 (numerical check) is of shape (4, 1) and has values\n",
      " array([[ 0.07060935],\n",
      "       [ 0.06811386],\n",
      "       [-0.07341947],\n",
      "       [-0.02507312]])\n",
      "\n",
      "the respective maximum differences:\n",
      "1.9276619286912045e-08\n",
      "3.196706895025603e-08\n",
      "1.7845128250093545e-08\n",
      "1.779303084037398e-08\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "#choose value to use (x or X) depending on type of input (single sample or batch)\n",
    "x_n = x\n",
    "\n",
    "y_pred = NNet.propagate(x_n)\n",
    "L = NNet.cost_funct(y_pred)\n",
    "\n",
    "dL_dW2_n = np.zeros(W2.shape)\n",
    "for r in range(W2.shape[0]):\n",
    "    for c in range(W2.shape[1]):\n",
    "        NNet.W2[r,c] += eps\n",
    "        y_pred = NNet.propagate(x_n)\n",
    "        L_eps = NNet.cost_funct(y_pred)\n",
    "        NNet.W2[r,c] -= eps\n",
    "\n",
    "        dL_dW2_n[r,c] = (L_eps-L)/eps\n",
    "\n",
    "print_value(dL_dW2_n,'dL_dW2 (numerical check)')\n",
    "\n",
    "dL_db2_n = np.zeros(b2.shape)\n",
    "for r in range(b2.shape[0]):\n",
    "    NNet.b2[r,0] += eps\n",
    "    y_pred = NNet.propagate(x_n)\n",
    "    L_eps = NNet.cost_funct(y_pred)\n",
    "    NNet.b2[r,0] -= eps\n",
    "\n",
    "    dL_db2_n[r,0] = (L_eps-L)/eps\n",
    "\n",
    "print_value(dL_db2_n,'dL_db2 (numerical check)')\n",
    "\n",
    "dL_dW1_n = np.zeros(W1.shape)\n",
    "for r in range(W1.shape[0]):\n",
    "    for c in range(W1.shape[1]):\n",
    "        NNet.W1[r,c] += eps\n",
    "        y_pred = NNet.propagate(x_n)\n",
    "        L_eps = NNet.cost_funct(y_pred)\n",
    "        NNet.W1[r,c] -= eps\n",
    "\n",
    "        dL_dW1_n[r,c] = (L_eps-L)/eps\n",
    "\n",
    "print_value(dL_dW1_n,'dL_dW1 (numerical check)')\n",
    "\n",
    "dL_db1_n = np.zeros(b1.shape)\n",
    "for r in range(b1.shape[0]):\n",
    "    NNet.b1[r,0] += eps\n",
    "    y_pred = NNet.propagate(x_n)\n",
    "    L_eps = NNet.cost_funct(y_pred)\n",
    "    NNet.b1[r,0] -= eps\n",
    "\n",
    "    dL_db1_n[r,0] = (L_eps-L)/eps\n",
    "\n",
    "print_value(dL_db1_n,'dL_db1 (numerical check)')\n",
    "\n",
    "#check the difference\n",
    "print('\\nthe respective maximum differences:')\n",
    "print(np.max(np.abs(dL_dW2_n - dL_dW2)))\n",
    "print(np.max(np.abs(dL_db2_n - dL_db2)))\n",
    "print(np.max(np.abs(dL_dW1_n - dL_dW1)))\n",
    "print(np.max(np.abs(dL_db1_n - dL_db1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f293b-27a4-4cb7-98a1-76542d8347ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
